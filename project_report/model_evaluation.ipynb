{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import pipeline, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paradigms in the BLiMP dataset\n",
    "paradigms = ['adjunct_island', \n",
    "             'anaphor_gender_agreement', \n",
    "             'anaphor_number_agreement', \n",
    "             'animate_subject_passive', \n",
    "             'animate_subject_trans', \n",
    "             'causative', \n",
    "             'complex_NP_island', \n",
    "             'coordinate_structure_constraint_complex_left_branch', \n",
    "             'coordinate_structure_constraint_object_extraction', \n",
    "             'determiner_noun_agreement_1', \n",
    "             'determiner_noun_agreement_2', \n",
    "             'determiner_noun_agreement_irregular_1', \n",
    "             'determiner_noun_agreement_irregular_2', \n",
    "             'determiner_noun_agreement_with_adj_2', \n",
    "             'determiner_noun_agreement_with_adj_irregular_1', \n",
    "             'determiner_noun_agreement_with_adj_irregular_2', \n",
    "             'determiner_noun_agreement_with_adjective_1', \n",
    "             'distractor_agreement_relational_noun', \n",
    "             'distractor_agreement_relative_clause', \n",
    "             'drop_argument', \n",
    "             'ellipsis_n_bar_1', \n",
    "             'ellipsis_n_bar_2', \n",
    "             'existential_there_object_raising', \n",
    "             'existential_there_quantifiers_1', \n",
    "             'existential_there_quantifiers_2', \n",
    "             'existential_there_subject_raising', \n",
    "             'expletive_it_object_raising', \n",
    "             'inchoative', \n",
    "             'intransitive', \n",
    "             'irregular_past_participle_adjectives', \n",
    "             'irregular_past_participle_verbs', \n",
    "             'irregular_plural_subject_verb_agreement_1', \n",
    "             'irregular_plural_subject_verb_agreement_2', \n",
    "             'left_branch_island_echo_question', \n",
    "             'left_branch_island_simple_question', \n",
    "             #'matrix_question_npi_licensor_present', \n",
    "             'npi_present_1', \n",
    "             'npi_present_2', \n",
    "             'only_npi_licensor_present',\n",
    "             'only_npi_scope', \n",
    "             'passive_1', \n",
    "             'passive_2', \n",
    "             'principle_A_c_command', \n",
    "             'principle_A_case_1', \n",
    "             'principle_A_case_2', \n",
    "             'principle_A_domain_1', \n",
    "             'principle_A_domain_2', \n",
    "             'principle_A_domain_3', \n",
    "             'principle_A_reconstruction', \n",
    "             'regular_plural_subject_verb_agreement_1', \n",
    "             'regular_plural_subject_verb_agreement_2', \n",
    "             'sentential_negation_npi_licensor_present', \n",
    "             'sentential_negation_npi_scope', \n",
    "             'sentential_subject_island', \n",
    "             'superlative_quantifiers_1', \n",
    "             'superlative_quantifiers_2', \n",
    "             'tough_vs_raising_1', \n",
    "             'tough_vs_raising_2', \n",
    "             'transitive', \n",
    "             'wh_island', \n",
    "             'wh_questions_object_gap', \n",
    "             'wh_questions_subject_gap', \n",
    "             'wh_questions_subject_gap_long_distance', \n",
    "             'wh_vs_that_no_gap', \n",
    "             'wh_vs_that_no_gap_long_distance', \n",
    "             'wh_vs_that_with_gap', \n",
    "             'wh_vs_that_with_gap_long_distance']\n",
    "\n",
    "# A dictionary with abbveriations of lingustic terms for better display\n",
    "phenomena = {\n",
    "        \"anaphor_agreement\": \"ANA AGR\",\n",
    "        \"argument_structure\": \"ARG STR\",\n",
    "        \"binding\": \"BINDING\",\n",
    "        \"control_raising\": \"CTRL RAIS\",\n",
    "        \"determiner_noun_agreement\": \"D-N AGR\",\n",
    "        \"ellipsis\": \"ELLIPSIS\",\n",
    "        \"filler_gap_dependency\": \"FILLER. GAP\",\n",
    "        \"irregular_forms\": \"IRREGULAR\",\n",
    "        \"island_effects\": \"ISLAND\",\n",
    "        \"npi_licensing\": \"NPI\",\n",
    "        \"quantifiers\": \"QUANTIFIERS\",\n",
    "        \"subject_verb_agreement\": \"S-V AGR\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading coordinate_structure_constraint_complex_left_branch: 12.12%                              "
     ]
    }
   ],
   "source": [
    "# Create a dataframe to hold data for each paradigm\n",
    "blimp_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each paradigm and filter the dataset\n",
    "for paradigm in paradigms:\n",
    "    # Load the dataset for the current paradigm\n",
    "    subset = load_dataset('nyu-mll/BLiMP', name=paradigm)\n",
    "    \n",
    "    # Convert the dataset to a dataframe and add the paradigm column\n",
    "    subset_df = pd.DataFrame(subset['train'])\n",
    "    subset_df.rename(columns={'UID': 'paradigm'}, inplace=True)\n",
    "    # Add the phenomenon column using the phenomena dictionary\n",
    "    subset_df['phenomenon'] = subset_df['linguistics_term'].map(phenomena).fillna('EMPTY')\n",
    "\n",
    "    # Print loading status\n",
    "    print(f\"\\rLoading {paradigm}: {((paradigms.index(paradigm) + 1) / len(paradigms)) * 100:.2f}%{' ' * 30}\", end='')\n",
    "    \n",
    "    # Append the subset dataframe to the main dataframe\n",
    "    blimp_df = pd.concat([blimp_df, subset_df], ignore_index=True)\n",
    "\n",
    "# Display the combined dataframe\n",
    "blimp_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter entries where sentence_good is equal to sentence_bad\n",
    "identical_sentences = blimp_df[blimp_df['sentence_good'] == blimp_df['sentence_bad']]\n",
    "\n",
    "# Print a report\n",
    "print(f\"Number of identical sentences: {len(identical_sentences)}\")\n",
    "if not identical_sentences.empty:\n",
    "    print(\"Details of identical sentences:\")\n",
    "    print(identical_sentences[['sentence_good', 'paradigm']])\n",
    "else:\n",
    "    print(\"No identical sentences found.\")\n",
    "\n",
    "# Drop identical sentences from the dataframe\n",
    "blimp_df = blimp_df[blimp_df['sentence_good'] != blimp_df['sentence_bad']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "mbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Function to create a masked sentence with shared tokens and a single [MASK] for differences\n",
    "def process_tokens(row):\n",
    "    # Tokenize the good and bad sentences\n",
    "    good_tokens = row['sentence_good'].split()\n",
    "    bad_tokens = row['sentence_bad'].split()\n",
    "\n",
    "    # Remove common tokens from the beginning\n",
    "    common_start = []\n",
    "    while good_tokens and bad_tokens and good_tokens[0] == bad_tokens[0]:\n",
    "        common_start.append(good_tokens.pop(0))\n",
    "        bad_tokens.pop(0)\n",
    "\n",
    "    # Remove common tokens from the end\n",
    "    common_end = []\n",
    "    while good_tokens and bad_tokens and good_tokens[-1] == bad_tokens[-1]:\n",
    "        common_end.insert(0, good_tokens.pop(-1))\n",
    "        bad_tokens.pop(-1)\n",
    "\n",
    "\n",
    "    # If the good or bad tokens are empty, add a common token to both\n",
    "    if good_tokens == [] or bad_tokens == []:\n",
    "        additional_tokens = common_end.pop(0)\n",
    "        print(\"additional token:\", additional_tokens)\n",
    "        good_tokens.append(additional_tokens)\n",
    "        bad_tokens.append(additional_tokens)\n",
    "\n",
    "    # Collect the remaining tokens as the masked sentence\n",
    "    common_tokens = common_start + [\"[MASK]\"] + common_end\n",
    "    \n",
    "    good_fillers = mbert_tokenizer.tokenize(\" \".join(good_tokens))\n",
    "    bad_fillers = mbert_tokenizer.tokenize(\" \".join(bad_tokens))\n",
    "\n",
    "    sentence_masked = \" \".join(common_tokens)\n",
    "\n",
    "    # If the last token of the good and bad fillers is the same, add it to the masked sentence (same punctuation with MASK at the end)\n",
    "    if len(good_fillers) > 1 and len(bad_fillers) > 1: \n",
    "        if good_fillers[-1] == bad_fillers[-1]:\n",
    "            remove_punct = good_fillers.pop(-1)\n",
    "            sentence_masked += remove_punct\n",
    "            bad_fillers.pop(-1)\n",
    "\n",
    "    return sentence_masked, good_fillers, bad_fillers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the DataFrame\n",
    "blimp_df[['sentence_masked', 'good_fillers', 'bad_fillers']] = blimp_df.apply(lambda row: pd.Series(process_tokens(row)), axis=1)\n",
    "\n",
    "# Display a sample of the updated DataFrame\n",
    "blimp_df[['sentence_masked', 'good_fillers', 'bad_fillers']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where good_fillers or bad_fillers are empty\n",
    "empty_fillers = blimp_df[(blimp_df['good_fillers'].apply(len) == 0) | (blimp_df['bad_fillers'].apply(len) == 0)]\n",
    "\n",
    "# Print the filtered rows\n",
    "display(empty_fillers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the probabilities of the fillers to get the probability of the full sentence\n",
    "def get_filler_probability(model, sentence, filler):\n",
    "    total_probability = 1\n",
    "\n",
    "    # Iterate over the fillers and calculate the probability of each word\n",
    "    for i in range(0, len(filler)):\n",
    "        word = filler[i]\n",
    "        next_word = filler[i+1] if i+1 < len(filler) else \"\"\n",
    "\n",
    "        # Calculate the probability of the current word\n",
    "        word_probability = model(sentence, targets=[word])[0]['score']\n",
    "        total_probability *= word_probability\n",
    "\n",
    "        # Update the sentence with the filled word, pay attention to the ## tokens\n",
    "        if word.startswith(\"##\"):\n",
    "            if next_word.startswith('##'):\n",
    "                sentence = sentence.replace(\"[MASK]\", word[2:] + \"[MASK]\")\n",
    "            else:\n",
    "                sentence = sentence.replace(\"[MASK]\", word[2:] + \" [MASK]\")\n",
    "        else:\n",
    "            if next_word.startswith('##'):\n",
    "                sentence = sentence.replace(\"[MASK]\", word + \"[MASK]\")\n",
    "            else:\n",
    "                sentence = sentence.replace(\"[MASK]\", word + \" [MASK]\")\n",
    "\n",
    "        # Remove the [MASK] token if it is the last word\n",
    "        if i+1 == len(filler):\n",
    "            sentence = sentence.replace(\" [MASK]\", \"\")\n",
    "\n",
    "    return total_probability ** (1 / len(filler)) if len(filler) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_cased = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "mbert_cased = pipeline(\"fill-mask\", model=\"bert-base-multilingual-cased\")\n",
    "\n",
    "# different tokenizer needs different handling\n",
    "#bibert_cased = pipeline(\"fill-mask\", model=\"jhu-clsp/bibert-ende\", tokenizer=BertTokenizer.from_pretrained(\"jhu-clsp/bibert-ende\"))\n",
    "\n",
    "\n",
    "deploy_df = blimp_df.sample(100)\n",
    "# deploy_df = blimp_df.copy()\n",
    "\n",
    "# Calculate the probabilities for the good and bad fillers\n",
    "deploy_df['bert_good_prob'] = deploy_df.apply(\n",
    "    lambda row: get_filler_probability(mbert_cased, row['sentence_masked'], row['good_fillers']), axis=1\n",
    ")\n",
    "deploy_df['bert_bad_prob'] = deploy_df.apply(\n",
    "    lambda row: get_filler_probability(mbert_cased, row['sentence_masked'], row['bad_fillers']), axis=1\n",
    ")\n",
    "\n",
    "# Calculate the prediction based on the probabilities\n",
    "deploy_df['bert_prediction'] = deploy_df.apply(\n",
    "    lambda row: 1 if row['bert_good_prob'] > row['bert_bad_prob'] else 0, axis=1\n",
    ")\n",
    "\n",
    "display(deploy_df[['sentence_masked', 'good_fillers', 'bad_fillers', 'bert_good_prob', 'bert_bad_prob', 'bert_prediction']])\n",
    "deploy_df['bert_prediction'].value_counts()\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "deploy_df.to_csv(\"mbert_base_cased_prediction.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
