{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import pipeline, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paradigms in the BLiMP dataset\n",
    "paradigms = ['adjunct_island', \n",
    "             'anaphor_gender_agreement', \n",
    "             'anaphor_number_agreement', \n",
    "             'animate_subject_passive', \n",
    "             'animate_subject_trans', \n",
    "             'causative', \n",
    "             'complex_NP_island', \n",
    "             'coordinate_structure_constraint_complex_left_branch', \n",
    "             'coordinate_structure_constraint_object_extraction', \n",
    "             'determiner_noun_agreement_1', \n",
    "             'determiner_noun_agreement_2', \n",
    "             'determiner_noun_agreement_irregular_1', \n",
    "             'determiner_noun_agreement_irregular_2', \n",
    "             'determiner_noun_agreement_with_adj_2', \n",
    "             'determiner_noun_agreement_with_adj_irregular_1', \n",
    "             'determiner_noun_agreement_with_adj_irregular_2', \n",
    "             'determiner_noun_agreement_with_adjective_1', \n",
    "             'distractor_agreement_relational_noun', \n",
    "             'distractor_agreement_relative_clause', \n",
    "             'drop_argument', \n",
    "             'ellipsis_n_bar_1', \n",
    "             'ellipsis_n_bar_2', \n",
    "             'existential_there_object_raising', \n",
    "             'existential_there_quantifiers_1', \n",
    "             'existential_there_quantifiers_2', \n",
    "             'existential_there_subject_raising', \n",
    "             'expletive_it_object_raising', \n",
    "             'inchoative', \n",
    "             'intransitive', \n",
    "             'irregular_past_participle_adjectives', \n",
    "             'irregular_past_participle_verbs', \n",
    "             'irregular_plural_subject_verb_agreement_1', \n",
    "             'irregular_plural_subject_verb_agreement_2', \n",
    "             'left_branch_island_echo_question', \n",
    "             'left_branch_island_simple_question', \n",
    "             #'matrix_question_npi_licensor_present', \n",
    "             'npi_present_1', \n",
    "             'npi_present_2', \n",
    "             'only_npi_licensor_present',\n",
    "             'only_npi_scope', \n",
    "             'passive_1', \n",
    "             'passive_2', \n",
    "             'principle_A_c_command', \n",
    "             'principle_A_case_1', \n",
    "             'principle_A_case_2', \n",
    "             'principle_A_domain_1', \n",
    "             'principle_A_domain_2', \n",
    "             'principle_A_domain_3', \n",
    "             'principle_A_reconstruction', \n",
    "             'regular_plural_subject_verb_agreement_1', \n",
    "             'regular_plural_subject_verb_agreement_2', \n",
    "             'sentential_negation_npi_licensor_present', \n",
    "             'sentential_negation_npi_scope', \n",
    "             'sentential_subject_island', \n",
    "             'superlative_quantifiers_1', \n",
    "             'superlative_quantifiers_2', \n",
    "             'tough_vs_raising_1', \n",
    "             'tough_vs_raising_2', \n",
    "             'transitive', \n",
    "             'wh_island', \n",
    "             'wh_questions_object_gap', \n",
    "             'wh_questions_subject_gap', \n",
    "             'wh_questions_subject_gap_long_distance', \n",
    "             'wh_vs_that_no_gap', \n",
    "             'wh_vs_that_no_gap_long_distance', \n",
    "             'wh_vs_that_with_gap', \n",
    "             'wh_vs_that_with_gap_long_distance']\n",
    "\n",
    "# A dictionary with abbveriations of lingustic terms for better display\n",
    "phenomena = {\n",
    "        \"anaphor_agreement\": \"ANA AGR\",\n",
    "        \"argument_structure\": \"ARG STR\",\n",
    "        \"binding\": \"BINDING\",\n",
    "        \"control_raising\": \"CTRL RAIS\",\n",
    "        \"determiner_noun_agreement\": \"D-N AGR\",\n",
    "        \"ellipsis\": \"ELLIPSIS\",\n",
    "        \"filler_gap_dependency\": \"FILLER. GAP\",\n",
    "        \"irregular_forms\": \"IRREGULAR\",\n",
    "        \"island_effects\": \"ISLAND\",\n",
    "        \"npi_licensing\": \"NPI\",\n",
    "        \"quantifiers\": \"QUANTIFIERS\",\n",
    "        \"subject_verb_agreement\": \"S-V AGR\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading coordinate_structure_constraint_complex_left_branch: 12.12%                              "
     ]
<<<<<<< HEAD
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_good</th>\n",
       "      <th>sentence_bad</th>\n",
       "      <th>field</th>\n",
       "      <th>linguistics_term</th>\n",
       "      <th>paradigm</th>\n",
       "      <th>simple_LM_method</th>\n",
       "      <th>one_prefix_method</th>\n",
       "      <th>two_prefix_method</th>\n",
       "      <th>lexically_identical</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>phenomenon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8204</th>\n",
       "      <td>Who had Liam and Timothy discussed?</td>\n",
       "      <td>Who has Liam discussed and Timothy?</td>\n",
       "      <td>syntax</td>\n",
       "      <td>island_effects</td>\n",
       "      <td>coordinate_structure_constraint_object_extraction</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>204</td>\n",
       "      <td>ISLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14048</th>\n",
       "      <td>The vase impresses that excited woman.</td>\n",
       "      <td>The vase impresses that excited women.</td>\n",
       "      <td>morphology</td>\n",
       "      <td>determiner_noun_agreement</td>\n",
       "      <td>determiner_noun_agreement_with_adj_irregular_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>48</td>\n",
       "      <td>D-N AGR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8061</th>\n",
       "      <td>What will those people and Mary remember?</td>\n",
       "      <td>What did those people remember and Mary?</td>\n",
       "      <td>syntax</td>\n",
       "      <td>island_effects</td>\n",
       "      <td>coordinate_structure_constraint_object_extraction</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>ISLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27061</th>\n",
       "      <td>Some lamp wasn't dimming.</td>\n",
       "      <td>Some lamp wasn't lifting.</td>\n",
       "      <td>syntax</td>\n",
       "      <td>argument_structure</td>\n",
       "      <td>inchoative</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>61</td>\n",
       "      <td>ARG STR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30513</th>\n",
       "      <td>Claire wore these socks.</td>\n",
       "      <td>Claire worn these socks.</td>\n",
       "      <td>morphology</td>\n",
       "      <td>irregular_forms</td>\n",
       "      <td>irregular_past_participle_verbs</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>513</td>\n",
       "      <td>IRREGULAR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sentence_good  \\\n",
       "8204         Who had Liam and Timothy discussed?   \n",
       "14048     The vase impresses that excited woman.   \n",
       "8061   What will those people and Mary remember?   \n",
       "27061                  Some lamp wasn't dimming.   \n",
       "30513                   Claire wore these socks.   \n",
       "\n",
       "                                   sentence_bad       field  \\\n",
       "8204        Who has Liam discussed and Timothy?      syntax   \n",
       "14048    The vase impresses that excited women.  morphology   \n",
       "8061   What did those people remember and Mary?      syntax   \n",
       "27061                 Some lamp wasn't lifting.      syntax   \n",
       "30513                  Claire worn these socks.  morphology   \n",
       "\n",
       "                linguistics_term  \\\n",
       "8204              island_effects   \n",
       "14048  determiner_noun_agreement   \n",
       "8061              island_effects   \n",
       "27061         argument_structure   \n",
       "30513            irregular_forms   \n",
       "\n",
       "                                                paradigm  simple_LM_method  \\\n",
       "8204   coordinate_structure_constraint_object_extraction              True   \n",
       "14048     determiner_noun_agreement_with_adj_irregular_1              True   \n",
       "8061   coordinate_structure_constraint_object_extraction              True   \n",
       "27061                                         inchoative              True   \n",
       "30513                    irregular_past_participle_verbs              True   \n",
       "\n",
       "       one_prefix_method  two_prefix_method  lexically_identical  pair_id  \\\n",
       "8204               False              False                False      204   \n",
       "14048               True              False                 True       48   \n",
       "8061               False              False                False       61   \n",
       "27061              False              False                False       61   \n",
       "30513               True              False                False      513   \n",
       "\n",
       "      phenomenon  \n",
       "8204      ISLAND  \n",
       "14048    D-N AGR  \n",
       "8061      ISLAND  \n",
       "27061    ARG STR  \n",
       "30513  IRREGULAR  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
=======
>>>>>>> 59c41316d56d7a1801649370697fcfe80bc7a5d9
    }
   ],
   "source": [
    "# Create a dataframe to hold data for each paradigm\n",
    "blimp_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each paradigm and filter the dataset\n",
    "for paradigm in paradigms:\n",
    "    # Load the dataset for the current paradigm\n",
    "    subset = load_dataset('nyu-mll/BLiMP', name=paradigm)\n",
    "    \n",
    "    # Convert the dataset to a dataframe and add the paradigm column\n",
    "    subset_df = pd.DataFrame(subset['train'])\n",
    "    subset_df.rename(columns={'UID': 'paradigm'}, inplace=True)\n",
    "    # Add the phenomenon column using the phenomena dictionary\n",
    "    subset_df['phenomenon'] = subset_df['linguistics_term'].map(phenomena).fillna('EMPTY')\n",
    "\n",
    "    # Print loading status\n",
    "    print(f\"\\rLoading {paradigm}: {((paradigms.index(paradigm) + 1) / len(paradigms)) * 100:.2f}%{' ' * 30}\", end='')\n",
    "    \n",
    "    # Append the subset dataframe to the main dataframe\n",
    "    blimp_df = pd.concat([blimp_df, subset_df], ignore_index=True)\n",
    "\n",
    "# Display the combined dataframe\n",
    "blimp_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter entries where sentence_good is equal to sentence_bad\n",
    "identical_sentences = blimp_df[blimp_df['sentence_good'] == blimp_df['sentence_bad']]\n",
    "\n",
    "# Print a report\n",
    "print(f\"Number of identical sentences: {len(identical_sentences)}\")\n",
    "if not identical_sentences.empty:\n",
    "    print(\"Details of identical sentences:\")\n",
    "    print(identical_sentences[['sentence_good', 'paradigm']])\n",
    "else:\n",
    "    print(\"No identical sentences found.\")\n",
    "\n",
    "# Drop identical sentences from the dataframe\n",
    "blimp_df = blimp_df[blimp_df['sentence_good'] != blimp_df['sentence_bad']]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> 59c41316d56d7a1801649370697fcfe80bc7a5d9
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "mbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Function to create a masked sentence with shared tokens and a single [MASK] for differences\n",
    "def process_tokens(row):\n",
    "    # Tokenize the good and bad sentences\n",
    "    good_tokens = row['sentence_good'].split()\n",
    "    bad_tokens = row['sentence_bad'].split()\n",
    "\n",
    "    # Remove common tokens from the beginning\n",
    "    common_start = []\n",
    "    while good_tokens and bad_tokens and good_tokens[0] == bad_tokens[0]:\n",
    "        common_start.append(good_tokens.pop(0))\n",
    "        bad_tokens.pop(0)\n",
    "\n",
    "    # Remove common tokens from the end\n",
    "    common_end = []\n",
    "    while good_tokens and bad_tokens and good_tokens[-1] == bad_tokens[-1]:\n",
    "        common_end.insert(0, good_tokens.pop(-1))\n",
    "        bad_tokens.pop(-1)\n",
    "\n",
    "\n",
    "    # If the good or bad tokens are empty, add a common token to both\n",
    "    if good_tokens == [] or bad_tokens == []:\n",
    "        additional_tokens = common_end.pop(0)\n",
    "        print(\"additional token:\", additional_tokens)\n",
    "        good_tokens.append(additional_tokens)\n",
    "        bad_tokens.append(additional_tokens)\n",
    "\n",
    "    # Collect the remaining tokens as the masked sentence\n",
    "    common_tokens = common_start + [\"[MASK]\"] + common_end\n",
    "    \n",
    "    good_fillers = mbert_tokenizer.tokenize(\" \".join(good_tokens))\n",
    "    bad_fillers = mbert_tokenizer.tokenize(\" \".join(bad_tokens))\n",
    "\n",
    "    sentence_masked = \" \".join(common_tokens)\n",
    "\n",
    "    # If the last token of the good and bad fillers is the same, add it to the masked sentence (same punctuation with MASK at the end)\n",
    "    if len(good_fillers) > 1 and len(bad_fillers) > 1: \n",
    "        if good_fillers[-1] == bad_fillers[-1]:\n",
    "            remove_punct = good_fillers.pop(-1)\n",
    "            sentence_masked += remove_punct\n",
    "            bad_fillers.pop(-1)\n",
    "\n",
    "    return sentence_masked, good_fillers, bad_fillers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional token: by\n",
      "additional token: fired.\n",
      "additional token: Kevin's\n",
      "additional token: Tiffany.\n",
      "additional token: a\n",
      "additional token: Irene.\n",
      "additional token: these\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_masked</th>\n",
       "      <th>good_fillers</th>\n",
       "      <th>bad_fillers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32992</th>\n",
       "      <td>The [MASK] yawn.</td>\n",
       "      <td>[alumni]</td>\n",
       "      <td>[al, ##umnus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39985</th>\n",
       "      <td>Linda's lawyers are [MASK] by the Lutherans.</td>\n",
       "      <td>[watched]</td>\n",
       "      <td>[c, ##lash, ##ed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38840</th>\n",
       "      <td>[MASK] Suzanne will reveal ever turn.</td>\n",
       "      <td>[Only, a, lot, of, car, ##ts, that]</td>\n",
       "      <td>[A, lot, of, car, ##ts, that, only]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54269</th>\n",
       "      <td>[MASK] driver should kiss at most nine dancers.</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[No]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26180</th>\n",
       "      <td>Samuel [MASK] it to be acceptable that the Cli...</td>\n",
       "      <td>[couldn, ', t, want]</td>\n",
       "      <td>[is, press, ##uring]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37080</th>\n",
       "      <td>[MASK] the Clintons should ever wish to donate.</td>\n",
       "      <td>[Only]</td>\n",
       "      <td>[Even]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43836</th>\n",
       "      <td>Nina can think about herself [MASK] this high ...</td>\n",
       "      <td>[ins, ##ult, ##ing]</td>\n",
       "      <td>[ins, ##ulte, ##d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24034</th>\n",
       "      <td>[MASK] looking like a lot of sketches.</td>\n",
       "      <td>[Every, cas, ##sero, ##le, isn, ', t, there]</td>\n",
       "      <td>[There, isn, ', t, every, cas, ##sero, ##le]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19480</th>\n",
       "      <td>Raymond couldn't [MASK].</td>\n",
       "      <td>[attack]</td>\n",
       "      <td>[listen, to]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25585</th>\n",
       "      <td>There [MASK] to be fewer than three dresses di...</td>\n",
       "      <td>[turned, out]</td>\n",
       "      <td>[ne, ##gle, ##cted]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentence_masked  \\\n",
       "32992                                   The [MASK] yawn.   \n",
       "39985       Linda's lawyers are [MASK] by the Lutherans.   \n",
       "38840              [MASK] Suzanne will reveal ever turn.   \n",
       "54269    [MASK] driver should kiss at most nine dancers.   \n",
       "26180  Samuel [MASK] it to be acceptable that the Cli...   \n",
       "37080    [MASK] the Clintons should ever wish to donate.   \n",
       "43836  Nina can think about herself [MASK] this high ...   \n",
       "24034             [MASK] looking like a lot of sketches.   \n",
       "19480                           Raymond couldn't [MASK].   \n",
       "25585  There [MASK] to be fewer than three dresses di...   \n",
       "\n",
       "                                       good_fillers  \\\n",
       "32992                                      [alumni]   \n",
       "39985                                     [watched]   \n",
       "38840           [Only, a, lot, of, car, ##ts, that]   \n",
       "54269                                           [A]   \n",
       "26180                          [couldn, ', t, want]   \n",
       "37080                                        [Only]   \n",
       "43836                           [ins, ##ult, ##ing]   \n",
       "24034  [Every, cas, ##sero, ##le, isn, ', t, there]   \n",
       "19480                                      [attack]   \n",
       "25585                                 [turned, out]   \n",
       "\n",
       "                                        bad_fillers  \n",
       "32992                                 [al, ##umnus]  \n",
       "39985                             [c, ##lash, ##ed]  \n",
       "38840           [A, lot, of, car, ##ts, that, only]  \n",
       "54269                                          [No]  \n",
       "26180                          [is, press, ##uring]  \n",
       "37080                                        [Even]  \n",
       "43836                            [ins, ##ulte, ##d]  \n",
       "24034  [There, isn, ', t, every, cas, ##sero, ##le]  \n",
       "19480                                  [listen, to]  \n",
       "25585                           [ne, ##gle, ##cted]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to the DataFrame\n",
    "blimp_df[['sentence_masked', 'good_fillers', 'bad_fillers']] = blimp_df.apply(lambda row: pd.Series(process_tokens(row)), axis=1)\n",
    "\n",
    "# Display a sample of the updated DataFrame\n",
    "blimp_df[['sentence_masked', 'good_fillers', 'bad_fillers']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_good</th>\n",
       "      <th>sentence_bad</th>\n",
       "      <th>field</th>\n",
       "      <th>linguistics_term</th>\n",
       "      <th>paradigm</th>\n",
       "      <th>simple_LM_method</th>\n",
       "      <th>one_prefix_method</th>\n",
       "      <th>two_prefix_method</th>\n",
       "      <th>lexically_identical</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>phenomenon</th>\n",
       "      <th>sentence_masked</th>\n",
       "      <th>good_fillers</th>\n",
       "      <th>bad_fillers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentence_good, sentence_bad, field, linguistics_term, paradigm, simple_LM_method, one_prefix_method, two_prefix_method, lexically_identical, pair_id, phenomenon, sentence_masked, good_fillers, bad_fillers]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 59c41316d56d7a1801649370697fcfe80bc7a5d9
   "source": [
    "# Filter rows where good_fillers or bad_fillers are empty\n",
    "empty_fillers = blimp_df[(blimp_df['good_fillers'].apply(len) == 0) | (blimp_df['bad_fillers'].apply(len) == 0)]\n",
    "\n",
    "# Print the filtered rows\n",
    "display(empty_fillers)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": null,
>>>>>>> 59c41316d56d7a1801649370697fcfe80bc7a5d9
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the probabilities of the fillers to get the probability of the full sentence\n",
    "def get_filler_probability(model, sentence, filler):\n",
    "    total_probability = 1\n",
    "\n",
    "    # Iterate over the fillers and calculate the probability of each word\n",
    "    for i in range(0, len(filler)):\n",
    "        word = filler[i]\n",
    "        next_word = filler[i+1] if i+1 < len(filler) else \"\"\n",
    "\n",
    "        # Calculate the probability of the current word\n",
    "        word_probability = model(sentence, targets=[word])[0]['score']\n",
    "        total_probability *= word_probability\n",
    "\n",
    "        # Update the sentence with the filled word, pay attention to the ## tokens\n",
    "        if word.startswith(\"##\"):\n",
    "            if next_word.startswith('##'):\n",
    "                sentence = sentence.replace(\"[MASK]\", word[2:] + \"[MASK]\")\n",
    "            else:\n",
    "                sentence = sentence.replace(\"[MASK]\", word[2:] + \" [MASK]\")\n",
    "        else:\n",
    "            if next_word.startswith('##'):\n",
    "                sentence = sentence.replace(\"[MASK]\", word + \"[MASK]\")\n",
    "            else:\n",
    "                sentence = sentence.replace(\"[MASK]\", word + \" [MASK]\")\n",
    "\n",
    "        # Remove the [MASK] token if it is the last word\n",
    "        if i+1 == len(filler):\n",
    "            sentence = sentence.replace(\" [MASK]\", \"\")\n",
    "\n",
    "    return total_probability ** (1 / len(filler)) if len(filler) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_masked</th>\n",
       "      <th>good_fillers</th>\n",
       "      <th>bad_fillers</th>\n",
       "      <th>bert_good_prob</th>\n",
       "      <th>bert_bad_prob</th>\n",
       "      <th>bert_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who should Derek hug [MASK]?</td>\n",
       "      <td>[after, shock, ##ing, Richard]</td>\n",
       "      <td>[Richard, after, shock, ##ing]</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What had Theresa walked through [MASK]?</td>\n",
       "      <td>[while, talking, about, that, high, school]</td>\n",
       "      <td>[that, high, school, while, talking, about]</td>\n",
       "      <td>0.012896</td>\n",
       "      <td>0.009715</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who will Katherine discover [MASK]?</td>\n",
       "      <td>[without, hiri, ##ng, Erin]</td>\n",
       "      <td>[Erin, without, hiri, ##ng]</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who has Colleen aggravated [MASK]?</td>\n",
       "      <td>[before, kis, ##sing, Judy]</td>\n",
       "      <td>[Judy, before, kis, ##sing]</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What could a lot of cats break [MASK]?</td>\n",
       "      <td>[while, finding, all, convert, ##ibles]</td>\n",
       "      <td>[all, convert, ##ibles, while, finding]</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65995</th>\n",
       "      <td>A lot of boys remember [MASK] a report that di...</td>\n",
       "      <td>[what]</td>\n",
       "      <td>[that]</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.060339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65996</th>\n",
       "      <td>Some committees knew [MASK] the actresses that...</td>\n",
       "      <td>[who]</td>\n",
       "      <td>[that]</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65997</th>\n",
       "      <td>All governments forgot [MASK] these deer that ...</td>\n",
       "      <td>[what]</td>\n",
       "      <td>[that]</td>\n",
       "      <td>0.076872</td>\n",
       "      <td>0.578793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65998</th>\n",
       "      <td>Every patient didn't remember [MASK] these wai...</td>\n",
       "      <td>[who]</td>\n",
       "      <td>[that]</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.210847</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65999</th>\n",
       "      <td>This company was discovering [MASK] some libra...</td>\n",
       "      <td>[who]</td>\n",
       "      <td>[that]</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.307386</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65993 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentence_masked  \\\n",
       "0                           Who should Derek hug [MASK]?   \n",
       "1                What had Theresa walked through [MASK]?   \n",
       "2                    Who will Katherine discover [MASK]?   \n",
       "3                     Who has Colleen aggravated [MASK]?   \n",
       "4                 What could a lot of cats break [MASK]?   \n",
       "...                                                  ...   \n",
       "65995  A lot of boys remember [MASK] a report that di...   \n",
       "65996  Some committees knew [MASK] the actresses that...   \n",
       "65997  All governments forgot [MASK] these deer that ...   \n",
       "65998  Every patient didn't remember [MASK] these wai...   \n",
       "65999  This company was discovering [MASK] some libra...   \n",
       "\n",
       "                                      good_fillers  \\\n",
       "0                   [after, shock, ##ing, Richard]   \n",
       "1      [while, talking, about, that, high, school]   \n",
       "2                      [without, hiri, ##ng, Erin]   \n",
       "3                      [before, kis, ##sing, Judy]   \n",
       "4          [while, finding, all, convert, ##ibles]   \n",
       "...                                            ...   \n",
       "65995                                       [what]   \n",
       "65996                                        [who]   \n",
       "65997                                       [what]   \n",
       "65998                                        [who]   \n",
       "65999                                        [who]   \n",
       "\n",
       "                                       bad_fillers  bert_good_prob  \\\n",
       "0                   [Richard, after, shock, ##ing]        0.001766   \n",
       "1      [that, high, school, while, talking, about]        0.012896   \n",
       "2                      [Erin, without, hiri, ##ng]        0.000119   \n",
       "3                      [Judy, before, kis, ##sing]        0.000748   \n",
       "4          [all, convert, ##ibles, while, finding]        0.000541   \n",
       "...                                            ...             ...   \n",
       "65995                                       [that]        0.000099   \n",
       "65996                                       [that]        0.000349   \n",
       "65997                                       [that]        0.076872   \n",
       "65998                                       [that]        0.002038   \n",
       "65999                                       [that]        0.000108   \n",
       "\n",
       "       bert_bad_prob  bert_prediction  \n",
       "0           0.001394                1  \n",
       "1           0.009715                1  \n",
       "2           0.000289                0  \n",
       "3           0.000392                1  \n",
       "4           0.000341                1  \n",
       "...              ...              ...  \n",
       "65995       0.060339                0  \n",
       "65996       0.003272                0  \n",
       "65997       0.578793                0  \n",
       "65998       0.210847                0  \n",
       "65999       0.307386                0  \n",
       "\n",
       "[65993 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 59c41316d56d7a1801649370697fcfe80bc7a5d9
   "source": [
    "#bert_cased = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "mbert_cased = pipeline(\"fill-mask\", model=\"bert-base-multilingual-cased\")\n",
    "\n",
    "# different tokenizer needs different handling\n",
    "#bibert_cased = pipeline(\"fill-mask\", model=\"jhu-clsp/bibert-ende\", tokenizer=BertTokenizer.from_pretrained(\"jhu-clsp/bibert-ende\"))\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "# deploy_df = blimp_df.sample(100)\n",
    "deploy_df = blimp_df.copy()\n",
=======
    "deploy_df = blimp_df.sample(100)\n",
    "# deploy_df = blimp_df.copy()\n",
>>>>>>> 59c41316d56d7a1801649370697fcfe80bc7a5d9
    "\n",
    "# Calculate the probabilities for the good and bad fillers\n",
    "deploy_df['bert_good_prob'] = deploy_df.apply(\n",
    "    lambda row: get_filler_probability(mbert_cased, row['sentence_masked'], row['good_fillers']), axis=1\n",
    ")\n",
    "deploy_df['bert_bad_prob'] = deploy_df.apply(\n",
    "    lambda row: get_filler_probability(mbert_cased, row['sentence_masked'], row['bad_fillers']), axis=1\n",
    ")\n",
    "\n",
    "# Calculate the prediction based on the probabilities\n",
    "deploy_df['bert_prediction'] = deploy_df.apply(\n",
    "    lambda row: 1 if row['bert_good_prob'] > row['bert_bad_prob'] else 0, axis=1\n",
    ")\n",
    "\n",
    "display(deploy_df[['sentence_masked', 'good_fillers', 'bad_fillers', 'bert_good_prob', 'bert_bad_prob', 'bert_prediction']])\n",
    "deploy_df['bert_prediction'].value_counts()\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "deploy_df.to_csv(\"mbert_base_cased_prediction.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
